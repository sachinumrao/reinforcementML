{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3mokIIhf19wy2OZTS5Eqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinumrao/reinforcementML/blob/master/Pytorch_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWxdtBgOi-Ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "78b78445-4e62-4104-f9cf-7b58c84f6342"
      },
      "source": [
        "!nvcc -V"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SM_-aTKi1w1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e81959cc-abd8-47fa-a812-b43cc6001eca"
      },
      "source": [
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!conda install pytorch torchvision cudatoolkit=10.1 -c pytorch -y"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (6.2.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]) (0.16.0)\n",
            "/bin/bash: conda: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOYRTyaIjKWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dependencies\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Deep-Q network implementation : Neural Network as function approximator \n",
        "# for value function\n",
        "class DeepQModel(nn.Module):\n",
        "    def __init__(self, alpha):\n",
        "        super(DeepQModel, self).__init__()\n",
        "\n",
        "        # Convolutional layers for model\n",
        "        self.conv1 = nn.Conv2d(1, 32, 8, stride=4, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "\n",
        "        # Fully connected layers for model\n",
        "        self.fc1 = nn.Linear(128*19*8, 512)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "\n",
        "        # Define optimizer and loss\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "        # Check for compute devices\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        # Put model onto compute device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # Convert observation (image) to tensor\n",
        "        obs = T.Tensor(obs).to(self.device)\n",
        "\n",
        "        # Reshape image\n",
        "        obs = obs.view(-1, 1, 185, 95)\n",
        "\n",
        "        # Pass observation through convolutional layers\n",
        "        out = F.relu(self.conv1(obs))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.relu(self.conv3(out))\n",
        "\n",
        "        # Reshape (flatten) the convolutional output before passing it to \n",
        "        # fully connected layers\n",
        "        out = out.view(-1, 128*19*8)\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        out = F.relu(self.fc1(out))\n",
        "        actions = self.fc2(out)\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "# Implement RL agent\n",
        "class Agent(object):\n",
        "    def __init__(self, gamma, eps, alpha, max_mem, eps_end=0.05,\n",
        "                    replace=10000, action_space=[0, 1, 2, 3, 4, 5]):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps = eps\n",
        "        self.eps_end = eps_end\n",
        "        self.action_space = action_space\n",
        "        self.mem_size = max_mem\n",
        "        self.steps = 0\n",
        "        self.learn_step_counter = 0\n",
        "        self.memory = []\n",
        "        self.mem_counter = 0\n",
        "        self.replace_target_count = replace\n",
        "        self.Q_eval = DeepQModel(alpha)\n",
        "        self.Q_next = DeepQModel(alpha)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_):\n",
        "        if self.mem_counter < self.mem_size:\n",
        "            self.memory.append([state, action, reward, state_])\n",
        "        else:\n",
        "            self.memory[self.mem_counter % self.mem_size] = [state,\n",
        "                action, reward, state_]\n",
        "\n",
        "        self.mem_counter += 1\n",
        "\n",
        "    def choose_action(self, obs):\n",
        "        rand = np.random.random()\n",
        "        actions = self.Q_eval.forward(obs)\n",
        "\n",
        "        if rand < 1 - self.eps:\n",
        "            action = T.argmax(actions[1]).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "\n",
        "        self.steps += 1\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "\n",
        "        # Copy Q_Eval to Q_Next \n",
        "        if self.replace_target_count is not None and \\\n",
        "            self.learn_step_counter % self.replace_target_count == 0:\n",
        "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
        "\n",
        "        # Select data from memory \n",
        "        if self.mem_counter + batch_size < self.mem_size:\n",
        "            mem_start = int(np.random.choice(range(self.mem_counter)))\n",
        "        else:\n",
        "            mem_start = int(np.random.choice(range(self.mem_counter - \\\n",
        "                batch_size - 1)))\n",
        "\n",
        "        mini_batch = self.memory[mem_start:mem_start+batch_size]\n",
        "        memory = np.array(mini_batch)\n",
        "\n",
        "        # Q-Learning algorithm\n",
        "        q_pred = self.Q_eval.forward(\n",
        "            list(memory[:,0])).to(self.Q_eval.device)\n",
        "\n",
        "        q_next = self.Q_next.forward(\n",
        "            list(memory[:,3])).to(self.Q_eval.device)\n",
        "\n",
        "        max_action = T.argmax(q_next, dim=1).to(self.Q_eval.device)\n",
        "        reward = T.Tensor(list(memory[:, 2])).to(self.Q_eval.device)\n",
        "\n",
        "        q_target = q_pred\n",
        "        # print(q_pred.shape)\n",
        "        # print(q_next.shape)\n",
        "        q_target[:, max_action] = reward + self.gamma * T.max(q_next[1])\n",
        "\n",
        "        # Reduce eps (exploration factor)\n",
        "        if self.steps > 500:\n",
        "            if self.eps - 1e-4 > self.eps_end:\n",
        "                self.eps -= 1e-4\n",
        "            else:\n",
        "                self.eps = self.eps_end\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = self.Q_eval.loss(q_target, q_pred).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "        self.learn_step_counter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq2xW7cBjXfM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "7fd13469-824e-4512-dbfb-4fa098c5c62e"
      },
      "source": [
        "import gym\n",
        "#from DeepQModel import DeepQModel, Agent\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"Starting the simulation...\")\n",
        "env = gym.make(\"SpaceInvaders-v0\")\n",
        "agent = Agent(gamma=0.99, \n",
        "                eps=0.99,\n",
        "                alpha=0.01,\n",
        "                max_mem=5000,\n",
        "                replace=None)\n",
        "\n",
        "print(\"Environment is built...\")\n",
        "\n",
        "while agent.mem_counter < agent.mem_size:\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        obs_, reward, done, info = env.step(action)\n",
        "        if done and info['ale.lives'] == 0:\n",
        "            reward = -100\n",
        "\n",
        "        agent.store_transition(np.mean(obs[15:200, 30:125], axis=2),\n",
        "            action, reward, np.mean(obs_[15:200, 30:125], axis=2))\n",
        "\n",
        "        obs = obs_\n",
        "\n",
        "    print(\"Memory Initialized...\")\n",
        "\n",
        "    score_history = []\n",
        "    eps_history = []\n",
        "    num_games = 10\n",
        "    batch_size = 32\n",
        "\n",
        "    for i in range(num_games):\n",
        "        print('Starting Game: ', i+1, ' Epsilon: %.4f' % agent.eps)\n",
        "        eps_history.append(agent.eps)\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        frames = [np.sum(obs[15:200, 30:125], axis=2)]\n",
        "        score = 0\n",
        "        last_action = 0\n",
        "\n",
        "        while not done:\n",
        "            if len(frames) == 3:\n",
        "                action = agent.choose_action(frames)\n",
        "                frames = []\n",
        "            else:\n",
        "                action = last_action\n",
        "\n",
        "            obs_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            frames.append(np.sum(obs[15:200, 30:125], axis=2))\n",
        "\n",
        "            if done and info['ale.lives'] == 0:\n",
        "                reward = -100\n",
        "\n",
        "            agent.store_transition(np.mean(obs[15:200, 30:125], axis=2),\n",
        "                                    action,\n",
        "                                    reward,\n",
        "                                    np.mean(obs_[15:200, 30:125], axis=2)) \n",
        "            obs  = obs_\n",
        "            agent.learn(batch_size)\n",
        "            last_action = action\n",
        "\n",
        "        score_history.append(score)\n",
        "        print(\"Score: \", score)\n",
        "\n",
        "    x = [i+1 for i in range(num_games)]\n",
        "    \n",
        "    plt.plot(score_history)\n",
        "    plt.show()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting the simulation...\n",
            "Environment is built...\n",
            "Memory Initialized...\n",
            "Starting Game:  1  Epsilon: 0.9900\n",
            "Score:  110.0\n",
            "Starting Game:  2  Epsilon: 0.9900\n",
            "Score:  155.0\n",
            "Starting Game:  3  Epsilon: 0.9900\n",
            "Score:  155.0\n",
            "Starting Game:  4  Epsilon: 0.9366\n",
            "Score:  80.0\n",
            "Starting Game:  5  Epsilon: 0.8726\n",
            "Score:  25.0\n",
            "Starting Game:  6  Epsilon: 0.8265\n",
            "Score:  140.0\n",
            "Starting Game:  7  Epsilon: 0.7292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-369cd5711614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                                     np.mean(obs_[15:200, 30:125], axis=2)) \n\u001b[1;32m     65\u001b[0m             \u001b[0mobs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mobs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mlast_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-80b6a42130db>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# print(q_pred.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# print(q_next.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mq_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Reduce eps (exploration factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRgrg8s6jXm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}